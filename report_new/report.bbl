\begin{thebibliography}{1}

\bibitem{projectionFeasibility}
Andrei~Patrascu Ion~Necoara, Peter~Richtárik.
\newblock Randomized projection methods for convex feasibility.
\newblock 2017.
\newblock \url{http://141.85.225.150/papers/feasibility.pdf}.

\bibitem{kaczmarz}
Roman~Vershynin Thomas~Strohmer.
\newblock randomized kaczmarz algorithm with exponential convergence.
\newblock 2007.
\newblock \url{www.arxiv.org/pdf/math/0702226.pdf}.

\bibitem{sketchAndProject}
Robert~Mansel Gower.
\newblock Sketch and project: Randomized iterative methods for linear systems
  and inverting matrices, 2016.
\newblock \url{www.arxiv.org/pdf/1612.06013.pdf}.

\bibitem{iterativeLinearSystems}
Peter~Richtárik Robert Mansel~Gower.
\newblock Randomized iterative methods for linear systems.
\newblock 2015.
\newblock \url{http://epubs.siam.org/doi/abs/10.1137/15M1025487}.

\bibitem{GD}
RadhaKrishna Ganti.
\newblock Convergence rate of gradient descent algorithm, 2015.
\newblock
  \url{www.rkganti.wordpress.com/2015/08/21/convergence-rate-of-gradient-descent-algorithm/}.

\bibitem{SGD}
Mark~Schmidt Simon Lacoste-Julien and Francis Bach.
\newblock A simpler approach to obtaining an $\mathcal{O}(1/t)$ convergence
  rate for the projected stochastic subgradient method.
\newblock 2012.
\newblock \url{www.arxiv.org/pdf/1212.2002v2.pdf}.

\bibitem{SAGA}
Francis~Bach Aaron~Defazio.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock 2014.
\newblock \url{www.di.ens.fr/~fbach/Defazio_NIPS2014.pdf}.

\bibitem{acceleration}
David~Barber Aleksandar~Botev, Guy~Lever.
\newblock Nesterov’s accelerated gradient and momentum as approximations to
  regularised update descent.
\newblock 2016.
\newblock \url{www.arxiv.org/pdf/1607.01981.pdf}.

\bibitem{kosto}
konstantin.
\newblock ?
\newblock 2018?
\newblock \url{?}

\end{thebibliography}
